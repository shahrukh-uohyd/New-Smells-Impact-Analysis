{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6adbbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#changed files using pydriller\n",
    "import os\n",
    "import csv\n",
    "from pydriller import Repository\n",
    "from git import Repo, GitCommandError\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "def detect_fault_inducing_commits(repo_path, release_pairs, output_dir=\"buggy_files_reports\"):\n",
    "    \"\"\"\n",
    "    Enhanced fault-inducing commit detection with better blame handling and debugging.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    git_repo = Repo(repo_path)\n",
    "    results = {}\n",
    "    project_name = repo_path.split(\"/\")[-1]\n",
    "    if project_name==\"vlc-android\":\n",
    "        project_name=\"vlc\"\n",
    "\n",
    "    for start_release, end_release in release_pairs:\n",
    "        try:\n",
    "            print(f\"\\nAnalyzing {start_release} to {end_release}\")\n",
    "            \n",
    "            # Get commit range\n",
    "            start_commit = git_repo.tags[start_release].commit\n",
    "            end_commit = git_repo.tags[end_release].commit\n",
    "            \n",
    "            csv_filename = os.path.join(output_dir, f\"sqlite-{start_release}.csv\")\n",
    "            buggy_files = set()\n",
    "            fix_count = 0\n",
    "            \n",
    "            with open(csv_filename, 'w', newline='') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=[\n",
    "                    'buggy_file_path', 'fix_commit_hash', 'fix_date',\n",
    "                    'buggy_commit_hash', 'buggy_date', 'fix_message'\n",
    "                ])\n",
    "                writer.writeheader()\n",
    "                \n",
    "                # Traverse commits with PyDriller\n",
    "                for commit in Repository(\n",
    "                    repo_path,\n",
    "                    since=start_commit.committed_datetime,\n",
    "                    to=end_commit.committed_datetime,\n",
    "                    only_modifications_with_file_types=['.java', '.cpp', '.c', '.h']  # Filter by file type\n",
    "                ).traverse_commits():\n",
    "                    \n",
    "#                     if not is_fix_commit(commit.msg):\n",
    "#                         continue\n",
    "                    \n",
    "                    fix_count += 1\n",
    "                    print(f\"  Fix commit: {commit.hash[:8]} - {commit.msg[:50]}...\")\n",
    "                    \n",
    "                    for modified_file in commit.modified_files:\n",
    "                        if not modified_file.new_path:\n",
    "                            continue\n",
    "                            \n",
    "                        try:\n",
    "                            # Get previous version of the file\n",
    "                            old_path = modified_file.old_path or modified_file.new_path\n",
    "                            previous_contents = git_repo.git.show(f\"{commit.hash}^:{old_path}\")\n",
    "                            current_contents = git_repo.git.show(f\"{commit.hash}:{modified_file.new_path}\")\n",
    "                            \n",
    "                            # Get changed lines\n",
    "                            diff = get_changed_lines(previous_contents, current_contents)\n",
    "                            if not diff:\n",
    "                                continue\n",
    "                                \n",
    "                            # Find blame for changed lines\n",
    "                            blame_output = git_repo.git.blame(\n",
    "                                '-w', '-l', '-p',  # -w ignores whitespace, -l shows long hashes\n",
    "                                f\"{commit.hash}^\",  # Look at parent commit\n",
    "                                '--', modified_file.new_path\n",
    "                            )\n",
    "                            \n",
    "                            buggy_commits = parse_blame_for_lines(blame_output, diff)\n",
    "                            \n",
    "                            for buggy_hash in buggy_commits:\n",
    "                                try:\n",
    "                                    buggy_commit = git_repo.commit(buggy_hash)\n",
    "                                    # Prefer old_path for fault-inducing context, fallback to new_path\n",
    "                                    buggy_file_path = modified_file.old_path or modified_file.new_path\n",
    "\n",
    "                                    buggy_files.add(buggy_file_path)\n",
    "                                    writer.writerow({\n",
    "                                        'buggy_file_path': buggy_file_path,\n",
    "                                        'fix_commit_hash': commit.hash,\n",
    "                                        'fix_date': commit.committer_date,\n",
    "                                        'buggy_commit_hash': buggy_hash,\n",
    "                                        'buggy_date': buggy_commit.committed_datetime,\n",
    "                                        'fix_message': commit.msg[:200].replace('\\n', ' ')\n",
    "                                    })\n",
    "\n",
    "                                    print(f\"    Found buggy commit: {buggy_hash[:8]} for {modified_file.new_path}\")\n",
    "                                    \n",
    "                                except Exception as e:\n",
    "                                    print(f\"    Error processing buggy commit: {str(e)}\")\n",
    "                                    continue\n",
    "                                    \n",
    "                        except GitCommandError as e:\n",
    "                            print(f\"    Error processing {modified_file.new_path}: {str(e)}\")\n",
    "                            continue\n",
    "            \n",
    "            print(f\"\\nSummary for {start_release} to {end_release}:\")\n",
    "            print(f\"  Fix commits analyzed: {fix_count}\")\n",
    "            print(f\"  Buggy files found: {len(buggy_files)}\")\n",
    "            results[(start_release, end_release)] = buggy_files\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {start_release}-{end_release}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_changed_lines(old_content, new_content):\n",
    "    \"\"\"Identify changed lines between two file versions\"\"\"\n",
    "    old_lines = old_content.splitlines()\n",
    "    new_lines = new_content.splitlines()\n",
    "    diff = []\n",
    "    \n",
    "    for i, (old_line, new_line) in enumerate(zip(old_lines, new_lines)):\n",
    "        if old_line != new_line:\n",
    "            diff.append(i+1)  # Line numbers start at 1\n",
    "    \n",
    "    # Handle added/removed lines at the end\n",
    "    len_diff = len(new_lines) - len(old_lines)\n",
    "    if len_diff > 0:\n",
    "        diff.extend(range(len(old_lines)+1, len(new_lines)+1))\n",
    "    \n",
    "    return diff\n",
    "\n",
    "def parse_blame_for_lines(blame_output, target_lines):\n",
    "    \"\"\"Parse blame output for specific line numbers.\"\"\"\n",
    "    commits = set()\n",
    "    current_line = 0\n",
    "    commit_hash = None  # Ensure this is always defined\n",
    "\n",
    "    for line in blame_output.split('\\n'):\n",
    "        if len(line) >= 40 and re.match(r'^[0-9a-f]{40}', line):  # New commit hash line\n",
    "            commit_hash = line.split()[0]\n",
    "        elif line.startswith('filename '):\n",
    "            current_line += 1\n",
    "        elif line.startswith('\\t'):\n",
    "            current_line += 1\n",
    "            if current_line in target_lines and commit_hash:\n",
    "                commits.add(commit_hash)\n",
    "\n",
    "    return commits\n",
    "\n",
    "\n",
    "def is_fix_commit(commit_message):\n",
    "    \"\"\"Enhanced fix commit detection\"\"\"\n",
    "    error_keywords = [\n",
    "        \"fix\", \"crash\", \"resolves\", \"regression\", \"fall back\", \"assertion\", \"coverity\",\n",
    "    \"reproducible\", \"stack-wanted\", \"steps-wanted\", \"testcase\", \"fail\", \"npe\", \"except\",\n",
    "    \"broken\", \"bug\", \"differential testing\", \"error\", \"address sanitizer\", \"hang\",\n",
    "    \"perma orange\", \"random orange\", \"intermittent\", \"steps to reproduce\", \"leak\",\n",
    "    \"stack trace\", \"heap overflow\", \"freeze\", \"problem\", \"overflow\", \"avoid\", \"issue\",\n",
    "    \"workaround\", \"break\", \"stop\"\n",
    "    ]\n",
    "    \n",
    "    lower_msg = commit_message.lower()\n",
    "    \n",
    "    # More sophisticated detection\n",
    "    has_bug_number = any(word.isdigit() for word in lower_msg.split())\n",
    "    has_issue_ref = '#' in lower_msg or 'issue' in lower_msg\n",
    "    \n",
    "    return any(kw in lower_msg for kw in error_keywords) or has_bug_number or has_issue_ref\n",
    "# Define the release pairs you want to analyze\n",
    "release_pairs_to_analyze = [\n",
    "#   javacpp  (\"0.5\", \"0.9\"),\n",
    "#         (\"0.9\", \"1.1\"),\n",
    "#         (\"1.1\", \"1.2\"),\n",
    "#         (\"1.2\", \"1.2.1\"),\n",
    "#         (\"1.2.1\", \"1.2.7\"),\n",
    "#         (\"1.2.7\", \"1.3\"),\n",
    "#         (\"1.3\", \"1.3.2\"),\n",
    "#         (\"1.3.2\", \"1.4\"),\n",
    "#         (\"1.4\", \"1.4.2\"),\n",
    "#         (\"1.4.2\", \"1.4.4\"),\n",
    "#         (\"1.4.4\", \"1.5\"),\n",
    "#         (\"1.5\", \"1.5.1-1\"),\n",
    "#         (\"1.5.1-1\", \"1.5.2\"),\n",
    "#       rocksdb (\"v5.0.2\", \"v5.4.6\"),\n",
    "#         (\"v5.4.6\", \"v5.6.2\"),\n",
    "#         (\"v5.6.2\", \"v5.9.2\"),\n",
    "#         (\"v5.9.2\", \"v5.11.2\"),\n",
    "#         (\"v5.11.2\", \"v5.14.3\"),\n",
    "#         (\"v5.14.3\", \"v5.17.2\"),\n",
    "#         (\"v5.17.2\", \"v5.18.3\"),\n",
    "#         (\"v5.18.3\", \"v6.1.1\"),\n",
    "#         (\"v6.1.1\", \"v6.2.2\"),\n",
    "#         (\"v6.2.2\", \"v6.2.4\")\n",
    "#    jpype   (\"v0.5.4.5\", \"v0.5.5.1\"),\n",
    "#         (\"v0.5.5.1\", \"v0.5.5.4\"),\n",
    "#         (\"v0.5.5.4\", \"v0.5.6\"),\n",
    "#         (\"v0.5.6\", \"v0.5.7\"),\n",
    "#         (\"v0.5.7\", \"v0.6.0\"),\n",
    "#         (\"v0.6.0\", \"v0.6.1\"),\n",
    "#         (\"v0.6.1\", \"v0.6.2\"),\n",
    "#         (\"v0.6.2\", \"v0.6.3\"),\n",
    "#         (\"v0.6.3\", \"v0.7\"),\n",
    "#         (\"v0.7\", \"v0.7.1\"),\n",
    "#         (\"v0.7.1\", \"v0.7.2\")\n",
    "#      realm-java  (\"v0.90.0\", \"v1.2.0\"),\n",
    "#        (\"v1.2.0\", \"v2.3.2\"),\n",
    "#         (\"v2.3.2\", \"v3.7.2\"),\n",
    "#         (\"v3.7.2\", \"v4.4.0\"),\n",
    "#         (\"v4.4.0\", \"v5.4.0\"),\n",
    "#         (\"v5.4.0\", \"v5.7.1\"),\n",
    "#         (\"v5.7.1\", \"v5.9.0\"),\n",
    "#         (\"v5.9.0\", \"v5.11.0\"),\n",
    "#         (\"v5.11.0\", \"v5.15.0\"),\n",
    "#         (\"v5.15.0\", \"v6.0.0\"),\n",
    "#      zstd-jni  (\"v0.4.4\", \"v1.3.0-1\"),\n",
    "#        (\"v1.3.0-1\", \"v1.3.2-2\"),\n",
    "#         (\"v1.3.2-2\", \"v1.3.3-1\"),\n",
    "#         (\"v1.3.3-1\", \"v1.3.4-1\"),\n",
    "#         (\"v1.3.4-1\", \"v1.3.4-8\"),\n",
    "#         (\"v1.3.4-8\", \"v1.3.5-3\"),\n",
    "#         (\"v1.3.5-3\", \"v1.3.7-1\"),\n",
    "#         (\"v1.3.7-1\", \"v1.3.8-1\"),\n",
    "#         (\"v1.3.8-1\", \"v1.4.0-1\"),\n",
    "#         (\"v1.4.0-1\", \"v1.4.2-1\"),\n",
    "#         (\"v1.4.2-1\", \"v1.4.4-3\")\n",
    "#     conscrypt  (\"1.0.0.RC2\", \"1.0.0.RC8\"),\n",
    "#         (\"1.0.0.RC8\", \"1.0.0.RC11\"),\n",
    "#         (\"1.0.0.RC11\", \"1.0.0.RC14\"),\n",
    "#         (\"1.0.0.RC14\", \"1.0.1\"),\n",
    "#         (\"1.0.1\", \"1.0.2\"),\n",
    "#         (\"1.0.2\", \"1.1.1\"),\n",
    "#         (\"1.1.1\", \"1.2.0\"),\n",
    "#         (\"1.2.0\", \"1.4.2\"),\n",
    "#         (\"1.4.2\", \"2.1.0\"),\n",
    "#         (\"2.1.0\", \"2.2.1\")\n",
    "#  java-smt   (\"0.1\", \"0.3\"),\n",
    "#     (\"0.3\", \"0.5\"),\n",
    "  #   (\"0.5\", \"0.60\"),\n",
    "#     (\"0.60\", \"1.0.1\"),\n",
    "#         (\"1.0.1\", \"2.0.0\"),\n",
    "#         (\"2.0.0\", \"2.0.0-alpha\"),\n",
    "#     (\"2.0.0-alpha\", \"2.2.0\"),\n",
    "     #    (\"2.2.0\", \"3.0.0\"),\n",
    "#     (\"3.0.0\", \"3.1.0\"),\n",
    "#         (\"3.1.0\", \"3.3.0\")\n",
    "# vlc  (\"2.5.4\", \"3.0.0\"),\n",
    "#     (\"3.0.0\", \"3.0.11\"),\n",
    "#         (\"3.0.11\", \"3.0.13\"),\n",
    "#         (\"3.0.13\", \"3.0.92\"),\n",
    "#         (\"3.0.92\", \"3.0.96\"),\n",
    "#         (\"3.0.96\", \"3.1.0\"),\n",
    "#         (\"3.1.0\", \"3.1.2\"),\n",
    "#         (\"3.1.2\", \"3.1.6\"),\n",
    "#         (\"3.1.6\", \"3.1.7\"),\n",
    "#         (\"3.1.7\", \"3.2.2\")\n",
    "# pljava    (\"V1_2_0\", \"V1_3_0\"),\n",
    "#     (\"V1_3_0\", \"V1_4_0\"),\n",
    "#     (\"V1_4_0\", \"V1_4_2\"),\n",
    "#     (\"V1_4_2\", \"V1_4_3\"),\n",
    "#   (\"V1_4_3\", \"REL1_5_STABLE-BASE\"),\n",
    "#         (\"REL1_5_STABLE-BASE\", \"V1_5_0b3\"),\n",
    "#         (\"V1_5_0b3\", \"V1_5_0\"),\n",
    "#         (\"V1_5_0\", \"V1_5_1b1\"),\n",
    "#         (\"V1_5_1b1\", \"V1_5_1b2\"),\n",
    "#         (\"V1_5_1b2\", \"V1_5_2\"),\n",
    "#         (\"V1_5_2\", \"V1_5_3\"),\n",
    "#         (\"V1_5_3\", \"V1_5_5\")\n",
    "   #sqlite\n",
    "    (\"3.42.0.1\", \"3.44.0.0\"),\n",
    "    (\"3.44.0.0\", \"3.45.0.0\"),\n",
    "    (\"3.45.0.0\", \"3.45.2.0\"),\n",
    "    (\"3.45.2.0\", \"3.46.0.0\"),\n",
    "    (\"3.46.0.0\", \"3.46.1.1\"),\n",
    "    (\"3.46.1.1\", \"3.47.0.0\"),\n",
    "    (\"3.47.0.0\", \"3.47.2.0\"),\n",
    "    (\"3.47.2.0\", \"3.49.0.0\"),\n",
    "    (\"3.49.0.0\", \"3.50.1.0\"),\n",
    "    (\"3.50.1.0\", \"3.50.3.0\")\n",
    "    #jni-bind\n",
    "#     (\"Release-0.8.0-alpha\",\"Release-0.9.1-alpha\"),\n",
    "#     (\"Release-0.9.1-alpha\",\"Release-0.9.3-alpha\"),\n",
    "#     (\"Release-0.9.3-alpha\",\"Release-0.9.6-alpha\"),\n",
    "#     (\"Release-0.9.6-alpha\",\"Release-0.9.7-alpha\"),\n",
    "#     (\"Release-0.9.7-alpha\",\"Release-0.9.8-alpha\"),\n",
    "#     (\"Release-0.9.8-alpha\",\"Release-0.9.9-alpha\"),\n",
    "#     (\"Release-0.9.9-alpha\",\"Release-1.0.0-beta\"),\n",
    "#     (\"Release-1.0.0-beta\",\"Release-1.1.0-beta\"),\n",
    "#     (\"Release-1.1.0-beta\",\"Release-1.1.2-beta\"),\n",
    "#     (\"Release-1.1.2-beta\",\"Release-1.2.3\")\n",
    "    #Monero-java\n",
    "    \n",
    "#     (\"v0.8.9\",\"v0.8.10\"),\n",
    "#     (\"v0.8.10\",\"v0.8.13\"),\n",
    "#     (\"v0.8.13\",\"v0.8.17\"),\n",
    "#     (\"v0.8.17\",\"v0.8.24\"),\n",
    "#     (\"v0.8.24\",\"v0.8.31\"),\n",
    "#     (\"v0.8.31\",\"v0.8.35\"),\n",
    "#     (\"v0.8.35\",\"v0.8.36\"),\n",
    "#     (\"v0.8.36\",\"v0.8.37\"),\n",
    "#     (\"v0.8.37\",\"v0.8.38\"),\n",
    "#     (\"v0.8.38\",\"v0.8.39\")\n",
    "    #webrtc\n",
    "#     (\"v0.2.0\", \"v0.3.0\"),\n",
    "#     (\"v0.3.0\", \"v0.4.0\"),\n",
    "#     (\"v0.4.0\", \"v0.6.0\"),\n",
    "#     (\"v0.6.0\", \"v0.7.0\"),\n",
    "#     (\"v0.7.0\", \"v0.8.0\"),\n",
    "#     (\"v0.8.0\", \"v0.10.0\"),\n",
    "#     (\"v0.10.0\", \"v0.11.0\"),\n",
    "#     (\"v0.11.0\", \"v0.12.0\"),\n",
    "#     (\"v0.12.0\", \"v0.13.0\"),\n",
    "#     (\"v0.13.0\", \"v0.14.0\")\n",
    "    #wolfcrypt\n",
    "#     (\"v1.0.0-stable\", \"v1.1.0-stable\"),\n",
    "#     (\"v1.1.0-stable\", \"v1.2.0-stable\"),\n",
    "#     (\"v1.2.0-stable\", \"v1.3.0-stable\"),\n",
    "#     (\"v1.3.0-stable\", \"v1.5.0-stable\"),\n",
    "#     (\"v1.5.0-stable\", \"v1.6.0-stable\"),\n",
    "#     (\"v1.6.0-stable\", \"v1.7.0-stable\"),\n",
    "#     (\"v1.7.0-stable\", \"v1.8.0-stable\")\n",
    "    #wolfssl\n",
    "#     (\"v1.4.0-stable\", \"v1.5.0-stable\"),\n",
    "#     (\"v1.5.0-stable\", \"v1.6.0-stable\"),\n",
    "#     (\"v1.6.0-stable\", \"v1.8.0-stable\"),\n",
    "#     (\"v1.8.0-stable\", \"v1.9.0-stable\"),\n",
    "#     (\"v1.9.0-stable\", \"v1.11.0-stable\"),\n",
    "#     (\"v1.11.0-stable\", \"v1.12.0-stable\"),\n",
    "#     (\"v1.12.0-stable\", \"v1.12.2\"),\n",
    "#     (\"v1.12.2\", \"v1.13.0-stable\"),\n",
    "#     (\"v1.13.0-stable\", \"v1.14.0-stable\"),\n",
    "#     (\"v1.14.0-stable\", \"v1.15.0-stable\")\n",
    "    \n",
    "]\n",
    "\n",
    "# Run the analysis\n",
    "results = detect_fault_inducing_commits(\n",
    "    repo_path=\"revision projects/sqlite/3.50.3.0\",\n",
    "    release_pairs=release_pairs_to_analyze,\n",
    "    output_dir=\"buggy_smelly/revision/general_changed\"\n",
    ")\n",
    "\n",
    "# Access results programmatically if needed\n",
    "for release_pair, buggy_files in results.items():\n",
    "    print(f\"Between {release_pair[0]} and {release_pair[1]}, found {len(buggy_files)} buggy files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822291e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#faulty files using pydriller and fault inducing commits\n",
    "import os\n",
    "import csv\n",
    "from pydriller import Repository\n",
    "from git import Repo, GitCommandError\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "def detect_fault_inducing_commits(repo_path, release_pairs, output_dir=\"buggy_files_reports\"):\n",
    "    \"\"\"\n",
    "    Enhanced fault-inducing commit detection with better blame handling and debugging.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    git_repo = Repo(repo_path)\n",
    "    results = {}\n",
    "    project_name = repo_path.split(\"/\")[-1]\n",
    "    if project_name==\"vlc-android\":\n",
    "        project_name=\"vlc\"\n",
    "\n",
    "    for start_release, end_release in release_pairs:\n",
    "        try:\n",
    "            print(f\"\\nAnalyzing {start_release} to {end_release}\")\n",
    "            \n",
    "            # Get commit range\n",
    "            start_commit = git_repo.tags[start_release].commit\n",
    "            end_commit = git_repo.tags[end_release].commit\n",
    "            \n",
    "            csv_filename = os.path.join(output_dir, f\"wolfssl-{start_release}.csv\")\n",
    "            buggy_files = set()\n",
    "            fix_count = 0\n",
    "            \n",
    "            with open(csv_filename, 'w', newline='') as csvfile:\n",
    "                writer = csv.DictWriter(csvfile, fieldnames=[\n",
    "                    'buggy_file_path', 'fix_commit_hash', 'fix_date',\n",
    "                    'buggy_commit_hash', 'buggy_date', 'fix_message'\n",
    "                ])\n",
    "                writer.writeheader()\n",
    "                \n",
    "                # Traverse commits with PyDriller\n",
    "                for commit in Repository(\n",
    "                    repo_path,\n",
    "                    since=start_commit.committed_datetime,\n",
    "                    to=end_commit.committed_datetime,\n",
    "                    only_modifications_with_file_types=['.java', '.cpp', '.c', '.h']  # Filter by file type\n",
    "                ).traverse_commits():\n",
    "                    \n",
    "                    if not is_fix_commit(commit.msg):\n",
    "                        continue\n",
    "                    \n",
    "                    fix_count += 1\n",
    "                    print(f\"  Fix commit: {commit.hash[:8]} - {commit.msg[:50]}...\")\n",
    "                    \n",
    "                    for modified_file in commit.modified_files:\n",
    "                        if not modified_file.new_path:\n",
    "                            continue\n",
    "                            \n",
    "                        try:\n",
    "                            # Get previous version of the file\n",
    "                            old_path = modified_file.old_path or modified_file.new_path\n",
    "                            previous_contents = git_repo.git.show(f\"{commit.hash}^:{old_path}\")\n",
    "                            current_contents = git_repo.git.show(f\"{commit.hash}:{modified_file.new_path}\")\n",
    "                            \n",
    "                            # Get changed lines\n",
    "                            diff = get_changed_lines(previous_contents, current_contents)\n",
    "                            if not diff:\n",
    "                                continue\n",
    "                                \n",
    "                            # Find blame for changed lines\n",
    "                            blame_output = git_repo.git.blame(\n",
    "                                '-w', '-l', '-p',  # -w ignores whitespace, -l shows long hashes\n",
    "                                f\"{commit.hash}^\",  # Look at parent commit\n",
    "                                '--', modified_file.new_path\n",
    "                            )\n",
    "                            \n",
    "                            buggy_commits = parse_blame_for_lines(blame_output, diff)\n",
    "                            \n",
    "                            for buggy_hash in buggy_commits:\n",
    "                                try:\n",
    "                                    buggy_commit = git_repo.commit(buggy_hash)\n",
    "                                    # Prefer old_path for fault-inducing context, fallback to new_path\n",
    "                                    buggy_file_path = modified_file.old_path or modified_file.new_path\n",
    "\n",
    "                                    buggy_files.add(buggy_file_path)\n",
    "                                    writer.writerow({\n",
    "                                        'buggy_file_path': buggy_file_path,\n",
    "                                        'fix_commit_hash': commit.hash,\n",
    "                                        'fix_date': commit.committer_date,\n",
    "                                        'buggy_commit_hash': buggy_hash,\n",
    "                                        'buggy_date': buggy_commit.committed_datetime,\n",
    "                                        'fix_message': commit.msg[:200].replace('\\n', ' ')\n",
    "                                    })\n",
    "\n",
    "                                    print(f\"    Found buggy commit: {buggy_hash[:8]} for {modified_file.new_path}\")\n",
    "                                    \n",
    "                                except Exception as e:\n",
    "                                    print(f\"    Error processing buggy commit: {str(e)}\")\n",
    "                                    continue\n",
    "                                    \n",
    "                        except GitCommandError as e:\n",
    "                            print(f\"    Error processing {modified_file.new_path}: {str(e)}\")\n",
    "                            continue\n",
    "            \n",
    "            print(f\"\\nSummary for {start_release} to {end_release}:\")\n",
    "            print(f\"  Fix commits analyzed: {fix_count}\")\n",
    "            print(f\"  Buggy files found: {len(buggy_files)}\")\n",
    "            results[(start_release, end_release)] = buggy_files\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {start_release}-{end_release}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "def get_changed_lines(old_content, new_content):\n",
    "    \"\"\"Identify changed lines between two file versions\"\"\"\n",
    "    old_lines = old_content.splitlines()\n",
    "    new_lines = new_content.splitlines()\n",
    "    diff = []\n",
    "    \n",
    "    for i, (old_line, new_line) in enumerate(zip(old_lines, new_lines)):\n",
    "        if old_line != new_line:\n",
    "            diff.append(i+1)  # Line numbers start at 1\n",
    "    \n",
    "    # Handle added/removed lines at the end\n",
    "    len_diff = len(new_lines) - len(old_lines)\n",
    "    if len_diff > 0:\n",
    "        diff.extend(range(len(old_lines)+1, len(new_lines)+1))\n",
    "    \n",
    "    return diff\n",
    "\n",
    "def parse_blame_for_lines(blame_output, target_lines):\n",
    "    \"\"\"Parse blame output for specific line numbers.\"\"\"\n",
    "    commits = set()\n",
    "    current_line = 0\n",
    "    commit_hash = None  # Ensure this is always defined\n",
    "\n",
    "    for line in blame_output.split('\\n'):\n",
    "        if len(line) >= 40 and re.match(r'^[0-9a-f]{40}', line):  # New commit hash line\n",
    "            commit_hash = line.split()[0]\n",
    "        elif line.startswith('filename '):\n",
    "            current_line += 1\n",
    "        elif line.startswith('\\t'):\n",
    "            current_line += 1\n",
    "            if current_line in target_lines and commit_hash:\n",
    "                commits.add(commit_hash)\n",
    "\n",
    "    return commits\n",
    "\n",
    "\n",
    "def is_fix_commit(commit_message):\n",
    "    \"\"\"Enhanced fix commit detection\"\"\"\n",
    "    error_keywords = [\n",
    "        \"fix\", \"crash\", \"resolves\", \"regression\", \"fall back\", \"assertion\", \"coverity\",\n",
    "    \"reproducible\", \"stack-wanted\", \"steps-wanted\", \"testcase\", \"fail\", \"npe\", \"except\",\n",
    "    \"broken\", \"bug\", \"differential testing\", \"error\", \"address sanitizer\", \"hang\",\n",
    "    \"perma orange\", \"random orange\", \"intermittent\", \"steps to reproduce\",\"assertion\", \"leak\",\n",
    "    \"stack trace\", \"heap overflow\", \"freez\",\"str\", \"problem\", \"overflow\", \"avoid\", \"issue\",\n",
    "    \"workaround\", \"break\", \"stop\"\n",
    "    ]\n",
    "    \n",
    "    lower_msg = commit_message.lower()\n",
    "    \n",
    "    # More sophisticated detection\n",
    "    has_bug_number = any(word.isdigit() for word in lower_msg.split())\n",
    "    has_issue_ref = '#' in lower_msg or 'issue' in lower_msg\n",
    "    \n",
    "    return any(kw in lower_msg for kw in error_keywords) or has_bug_number or has_issue_ref\n",
    "# Define the release pairs you want to analyze\n",
    "release_pairs_to_analyze = [\n",
    "#    javacpp (\"0.5\", \"0.9\"),\n",
    "#         (\"0.9\", \"1.1\"),\n",
    "#         (\"1.1\", \"1.2\"),\n",
    "#         (\"1.2\", \"1.2.1\"),\n",
    "#         (\"1.2.1\", \"1.2.7\"),\n",
    "#         (\"1.2.7\", \"1.3\"),\n",
    "#         (\"1.3\", \"1.3.2\"),\n",
    "#         (\"1.3.2\", \"1.4\"),\n",
    "#         (\"1.4\", \"1.4.2\"),\n",
    "#         (\"1.4.2\", \"1.4.4\"),\n",
    "#         (\"1.4.4\", \"1.5\"),\n",
    "#         (\"1.5\", \"1.5.1-1\"),\n",
    "#         (\"1.5.1-1\", \"1.5.2\"),\n",
    "\n",
    "    #     (\"v5.0.2\", \"v5.4.6\"),\n",
    "#         (\"v5.4.6\", \"v5.6.2\"),\n",
    "#         (\"v5.6.2\", \"v5.9.2\"),\n",
    "#         (\"v5.9.2\", \"v5.11.2\"),\n",
    "#         (\"v5.11.2\", \"v5.14.3\"),\n",
    "#         (\"v5.14.3\", \"v5.17.2\"),\n",
    "#         (\"v5.17.2\", \"v5.18.3\"),\n",
    "#         (\"v5.18.3\", \"v6.1.1\"),\n",
    "#         (\"v6.1.1\", \"v6.2.2\"),\n",
    "#         (\"v6.2.2\", \"v6.2.4\")\n",
    "#     jpype (\"v0.5.4.5\", \"v0.5.5.1\"),\n",
    "#         (\"v0.5.5.1\", \"v0.5.5.4\"),\n",
    "#         (\"v0.5.5.4\", \"v0.5.6\"),\n",
    "#         (\"v0.5.6\", \"v0.5.7\"),\n",
    "#         (\"v0.5.7\", \"v0.6.0\"),\n",
    "#         (\"v0.6.0\", \"v0.6.1\"),\n",
    "#         (\"v0.6.1\", \"v0.6.2\"),\n",
    "#         (\"v0.6.2\", \"v0.6.3\"),\n",
    "#         (\"v0.6.3\", \"v0.7\"),\n",
    "      #  (\"v0.7\", \"v0.7.1\")\n",
    "   # (\"v0.7.1\", \"v0.7.2\")\n",
    "  \n",
    "#     realm-java   (\"v0.90.0\", \"v1.2.0\"),\n",
    "#        (\"v1.2.0\", \"v2.3.2\"),\n",
    "#         (\"v2.3.2\", \"v3.7.2\"),\n",
    "#         (\"v3.7.2\", \"v4.4.0\"),\n",
    "#         (\"v4.4.0\", \"v5.4.0\"),\n",
    "#         (\"v5.4.0\", \"v5.7.1\"),\n",
    "#         (\"v5.7.1\", \"v5.9.0\"),\n",
    "#         (\"v5.9.0\", \"v5.11.0\"),\n",
    "#         (\"v5.11.0\", \"v5.15.0\"),\n",
    "#         (\"v5.15.0\", \"v6.0.0\"),\n",
    "#     zstd-jni   (\"v0.4.4\", \"v1.3.0-1\"),\n",
    "#        (\"v1.3.0-1\", \"v1.3.2-2\"),\n",
    "#         (\"v1.3.2-2\", \"v1.3.3-1\"),\n",
    "#         (\"v1.3.3-1\", \"v1.3.4-1\"),\n",
    "#         (\"v1.3.4-1\", \"v1.3.4-8\"),\n",
    "#         (\"v1.3.4-8\", \"v1.3.5-3\"),\n",
    "#         (\"v1.3.5-3\", \"v1.3.7-1\"),\n",
    "#         (\"v1.3.7-1\", \"v1.3.8-1\"),\n",
    "#         (\"v1.3.8-1\", \"v1.4.0-1\"),\n",
    "#         (\"v1.4.0-1\", \"v1.4.2-1\"),\n",
    "#         (\"v1.4.2-1\", \"v1.4.4-3\")\n",
    "#      conscrypt (\"1.0.0.RC2\", \"1.0.0.RC8\"),\n",
    "#         (\"1.0.0.RC8\", \"1.0.0.RC11\"),\n",
    "#         (\"1.0.0.RC11\", \"1.0.0.RC14\"),\n",
    "#         (\"1.0.0.RC14\", \"1.0.1\"),\n",
    "#         (\"1.0.1\", \"1.0.2\"),\n",
    "#         (\"1.0.2\", \"1.1.1\"),\n",
    "#         (\"1.1.1\", \"1.2.0\"),\n",
    "#         (\"1.2.0\", \"1.4.2\"),\n",
    "#         (\"1.4.2\", \"2.1.0\"),\n",
    "#         (\"2.1.0\", \"2.2.1\")\n",
    "# java-smt (\"0.1\", \"0.3\"),\n",
    "#     (\"0.3\", \"0.5\"),\n",
    " #   (\"0.5\", \"0.60\"),\n",
    "#     (\"0.60\", \"1.0.1\"),\n",
    "#         (\"1.0.1\", \"2.0.0\"),\n",
    "#         (\"2.0.0\", \"2.0.0-alpha\"),\n",
    "#     (\"2.0.0-alpha\", \"2.2.0\"),\n",
    "  #      (\"2.2.0\", \"3.0.0\"),\n",
    "#     (\"3.0.0\", \"3.1.0\"),\n",
    "#         (\"3.1.0\", \"3.3.0\")\n",
    "# vlv (\"2.5.4\", \"3.0.0\"),\n",
    "#     (\"3.0.0\", \"3.0.11\"),\n",
    "#         (\"3.0.11\", \"3.0.13\"),\n",
    "#         (\"3.0.13\", \"3.0.92\"),\n",
    "#         (\"3.0.92\", \"3.0.96\"),\n",
    "#         (\"3.0.96\", \"3.1.0\"),\n",
    "#         (\"3.1.0\", \"3.1.2\"),\n",
    "#         (\"3.1.2\", \"3.1.6\"),\n",
    "#         (\"3.1.6\", \"3.1.7\"),\n",
    "#         (\"3.1.7\", \"3.2.2\")\n",
    "# pljava  (\"V1_2_0\", \"V1_3_0\"),\n",
    "#     (\"V1_3_0\", \"V1_4_0\"),\n",
    "#     (\"V1_4_0\", \"V1_4_2\"),\n",
    "#     (\"V1_4_2\", \"V1_4_3\"),\n",
    "#   (\"V1_4_3\", \"REL1_5_STABLE-BASE\"),\n",
    "#         (\"REL1_5_STABLE-BASE\", \"V1_5_0b3\"),\n",
    "#         (\"V1_5_0b3\", \"V1_5_0\"),\n",
    "#         (\"V1_5_0\", \"V1_5_1b1\"),\n",
    "#         (\"V1_5_1b1\", \"V1_5_1b2\"),\n",
    "#         (\"V1_5_1b2\", \"V1_5_2\"),\n",
    "#         (\"V1_5_2\", \"V1_5_3\"),\n",
    "#         (\"V1_5_3\", \"V1_5_5\")\n",
    "    #sqlite\n",
    "#     (\"3.42.0.1\", \"3.44.0.0\"),\n",
    "#     (\"3.44.0.0\", \"3.45.0.0\"),\n",
    "#     (\"3.45.0.0\", \"3.45.2.0\"),\n",
    "#     (\"3.45.2.0\", \"3.46.0.0\"),\n",
    "#     (\"3.46.0.0\", \"3.46.1.1\"),\n",
    "#     (\"3.46.1.1\", \"3.47.0.0\"),\n",
    "#     (\"3.47.0.0\", \"3.47.2.0\"),\n",
    "#     (\"3.47.2.0\", \"3.49.0.0\"),\n",
    "#     (\"3.49.0.0\", \"3.50.1.0\"),\n",
    "#     (\"3.50.1.0\", \"3.50.3.0\")\n",
    "    #jni-bind\n",
    "#     (\"Release-0.8.0-alpha\",\"Release-0.9.1-alpha\"),\n",
    "#     (\"Release-0.9.1-alpha\",\"Release-0.9.3-alpha\"),\n",
    "#     (\"Release-0.9.3-alpha\",\"Release-0.9.6-alpha\"),\n",
    "#     (\"Release-0.9.6-alpha\",\"Release-0.9.7-alpha\"),\n",
    "#     (\"Release-0.9.7-alpha\",\"Release-0.9.8-alpha\"),\n",
    "#     (\"Release-0.9.8-alpha\",\"Release-0.9.9-alpha\"),\n",
    "#     (\"Release-0.9.9-alpha\",\"Release-1.0.0-beta\"),\n",
    "#     (\"Release-1.0.0-beta\",\"Release-1.1.0-beta\"),\n",
    "#     (\"Release-1.1.0-beta\",\"Release-1.1.2-beta\"),\n",
    "#     (\"Release-1.1.2-beta\",\"Release-1.2.3\")\n",
    "    #Monero-java\n",
    "    \n",
    "#     (\"v0.8.9\",\"v0.8.10\"),\n",
    "#     (\"v0.8.10\",\"v0.8.13\"),\n",
    "#     (\"v0.8.13\",\"v0.8.17\"),\n",
    "#     (\"v0.8.17\",\"v0.8.24\"),\n",
    "#     (\"v0.8.24\",\"v0.8.31\"),\n",
    "#     (\"v0.8.31\",\"v0.8.35\"),\n",
    "#     (\"v0.8.35\",\"v0.8.36\"),\n",
    "#     (\"v0.8.36\",\"v0.8.37\"),\n",
    "#     (\"v0.8.37\",\"v0.8.38\"),\n",
    "#     (\"v0.8.38\",\"v0.8.39\")\n",
    "    #webrtc\n",
    "#     (\"v0.2.0\", \"v0.3.0\"),\n",
    "#     (\"v0.3.0\", \"v0.4.0\"),\n",
    "#     (\"v0.4.0\", \"v0.6.0\"),\n",
    "#     (\"v0.6.0\", \"v0.7.0\"),\n",
    "#     (\"v0.7.0\", \"v0.8.0\"),\n",
    "#     (\"v0.8.0\", \"v0.10.0\"),\n",
    "#     (\"v0.10.0\", \"v0.11.0\"),\n",
    "#     (\"v0.11.0\", \"v0.12.0\"),\n",
    "#     (\"v0.12.0\", \"v0.13.0\"),\n",
    "#     (\"v0.13.0\", \"v0.14.0\")\n",
    "    #wolfcrypt\n",
    "#     (\"v1.0.0-stable\", \"v1.1.0-stable\"),\n",
    "#     (\"v1.1.0-stable\", \"v1.2.0-stable\"),\n",
    "#     (\"v1.2.0-stable\", \"v1.3.0-stable\"),\n",
    "#     (\"v1.3.0-stable\", \"v1.5.0-stable\"),\n",
    "#     (\"v1.5.0-stable\", \"v1.6.0-stable\"),\n",
    "#     (\"v1.6.0-stable\", \"v1.7.0-stable\"),\n",
    "#     (\"v1.7.0-stable\", \"v1.8.0-stable\")\n",
    "    #wolfssl\n",
    "    (\"v1.4.0-stable\", \"v1.5.0-stable\"),\n",
    "    (\"v1.5.0-stable\", \"v1.6.0-stable\"),\n",
    "    (\"v1.6.0-stable\", \"v1.8.0-stable\"),\n",
    "    (\"v1.8.0-stable\", \"v1.9.0-stable\"),\n",
    "    (\"v1.9.0-stable\", \"v1.11.0-stable\"),\n",
    "    (\"v1.11.0-stable\", \"v1.12.0-stable\"),\n",
    "    (\"v1.12.0-stable\", \"v1.12.2\"),\n",
    "    (\"v1.12.2\", \"v1.13.0-stable\"),\n",
    "    (\"v1.13.0-stable\", \"v1.14.0-stable\"),\n",
    "    (\"v1.14.0-stable\", \"v1.15.0-stable\"),\n",
    "    \n",
    "]\n",
    "\n",
    "# Run the analysis\n",
    "results = detect_fault_inducing_commits(\n",
    "    repo_path=\"revision projects/wolfssl/v1.15.0-stable\",\n",
    "    release_pairs=release_pairs_to_analyze,\n",
    "    output_dir=\"buggy_smelly/revision/general_faulty\"\n",
    ")\n",
    "\n",
    "# Access results programmatically if needed\n",
    "for release_pair, buggy_files in results.items():\n",
    "    print(f\"Between {release_pair[0]} and {release_pair[1]}, found {len(buggy_files)} buggy files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba816840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset for LR containing all 4 smells with their frequencies releasewise\n",
    "#containment\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------\n",
    "UNIVERSAL_FOLDERS = [\n",
    "    \"new smells/On extended dataset/Java files\",\n",
    "    \"new smells/On extended dataset/Native Java files\",\n",
    "    \"new smells/On extended dataset/Native C files\"\n",
    "]\n",
    "\n",
    "SMELL_DIRS = {\n",
    "    \"LanguageEnvy\": \"new smells/On extended dataset/LanguageEnvy\",\n",
    "    \"CrossRespDecl\": \"new smells/On extended dataset/crossLangDeclaration\",\n",
    "    \"ShotgunSurgery\": \"new smells/On extended dataset/ShotgunSurgery\",\n",
    "    \"EncapsulationBypass\": \"new smells/On extended dataset/EncapsulationBypass\"\n",
    "}\n",
    "\n",
    "METRICS_DIR = \"new smells/On extended dataset/codeChurn_LOC\"\n",
    "FAULTY_DIR = \"new smells/On extended dataset/general_faulty\"\n",
    "OUTPUT_DIR = \"new smells/On extended dataset/release wise fault dataset\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# HELPER FUNCTIONS\n",
    "# ----------------------------\n",
    "def load_smell_counts(csv_path):\n",
    "    \"\"\"Load smell CSV and count how many times each file appears.\"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        return {}\n",
    "    df = pd.read_csv(csv_path, dtype=str, keep_default_na=False)\n",
    "    file_col = [c for c in df.columns if \"file\" in c.lower()]\n",
    "    if not file_col:\n",
    "        return {}\n",
    "    file_col = file_col[0]\n",
    "    return df[file_col].value_counts().to_dict()  # file -> count\n",
    "\n",
    "\n",
    "def load_file_list(csv_path):\n",
    "    \"\"\"Load a set of file paths from a CSV file.\"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        return set()\n",
    "    df = pd.read_csv(csv_path, dtype=str, keep_default_na=False)\n",
    "    file_col = [c for c in df.columns if \"file\" in c.lower()]\n",
    "    if not file_col:\n",
    "        return set()\n",
    "    return set(df[file_col[0]].dropna().tolist())\n",
    "\n",
    "\n",
    "def load_metrics(csv_path):\n",
    "    \"\"\"Load LOC, Churn, and Previous Bugs metrics into a dict keyed by file path.\"\"\"\n",
    "    metrics = {}\n",
    "    if not os.path.exists(csv_path):\n",
    "        print(\"************************csv file doesnot exist\")\n",
    "        return metrics\n",
    "    df = pd.read_csv(csv_path, dtype=str, keep_default_na=False)\n",
    "    # find file column robustly\n",
    "    fcols = [c for c in df.columns if \"file\" in c.lower()]\n",
    "    if not fcols:\n",
    "        print(\"*********************filecol doesnot exist\")\n",
    "        return metrics\n",
    "    file_col = fcols[0]\n",
    "    for _, row in df.iterrows():\n",
    "        fname = str(row[file_col]).strip()\n",
    "        # try parse ints robustly\n",
    "        def safe_int(x):\n",
    "            try:\n",
    "                return int(float(x))\n",
    "            except Exception:\n",
    "                return 0\n",
    "        loc = safe_int(row.get(\"loc\", 0))\n",
    "        churn = safe_int(row.get(\"code_churn\", 0))\n",
    "        prev_fix = safe_int(row.get(\"previous_fixes\", 0))\n",
    "        metrics[fname] = (loc, churn, prev_fix)\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def gather_universal_file_set(project_name):\n",
    "    \"\"\"Combine file lists from Java, Native Java, and Native C sources.\"\"\"\n",
    "    all_files = set()\n",
    "    for folder in UNIVERSAL_FOLDERS:\n",
    "        csv_path = os.path.join(folder, f\"{project_name}.csv\")\n",
    "        if os.path.exists(csv_path):\n",
    "            df = pd.read_csv(csv_path, dtype=str, keep_default_na=False)\n",
    "            file_col = [c for c in df.columns if \"file\" in c.lower()]\n",
    "            if file_col:\n",
    "                all_files.update([x for x in df[file_col[0]].dropna().tolist()])\n",
    "    return list(all_files)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Matching helpers (containment-based)\n",
    "# ----------------------------\n",
    "def basename(path):\n",
    "    return os.path.basename(path) if path else path\n",
    "\n",
    "\n",
    "def smell_count_by_containment(smell_dict, universal_fname):\n",
    "    \"\"\"\n",
    "    Sum counts for smell keys that contain the universal filename or its basename.\n",
    "    smell_dict: {smell_path: count}\n",
    "    universal_fname: string (from universal file list)\n",
    "    \"\"\"\n",
    "    if not smell_dict or not universal_fname:\n",
    "        return 0\n",
    "    uni = universal_fname.strip()\n",
    "    uni_base = basename(uni)\n",
    "    total = 0\n",
    "    for k, v in smell_dict.items():\n",
    "        if not isinstance(k, str):\n",
    "            continue\n",
    "        k_str = k.strip()\n",
    "        # match if smell key contains the universal path OR the basename\n",
    "        if uni in k_str or uni_base and uni_base in k_str:\n",
    "            try:\n",
    "                total += int(v)\n",
    "            except Exception:\n",
    "                try:\n",
    "                    total += int(float(v))\n",
    "                except Exception:\n",
    "                    continue\n",
    "    return total\n",
    "\n",
    "\n",
    "def is_faulty_by_containment(faulty_set, universal_fname):\n",
    "    \"\"\"\n",
    "    Return True if any faulty path contains the universal filename (or its basename).\n",
    "    \"\"\"\n",
    "    if not faulty_set or not universal_fname:\n",
    "        return False\n",
    "    uni = universal_fname.strip()\n",
    "    uni_base = basename(uni)\n",
    "    for k in faulty_set:\n",
    "        if not isinstance(k, str):\n",
    "            continue\n",
    "        k_str = k.strip()\n",
    "        if uni in k_str or (uni_base and uni_base in k_str):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def find_metrics_by_containment(metrics_dict, universal_fname):\n",
    "    \"\"\"\n",
    "    Return metrics tuple for the best match:\n",
    "    1) exact match\n",
    "    2) if not found, find the first metrics key that contains the universal filename or vice versa, or contains basename.\n",
    "    \"\"\"\n",
    "    if not metrics_dict or not universal_fname:\n",
    "        return (0, 0, 0)\n",
    "    uni = universal_fname.strip()\n",
    "    uni_base = basename(uni)\n",
    "\n",
    "    # exact\n",
    "    if uni in metrics_dict:\n",
    "        return metrics_dict[uni]\n",
    "    # try basename exact\n",
    "    if uni_base in metrics_dict:\n",
    "        return metrics_dict[uni_base]\n",
    "\n",
    "    # try containment (metrics_key contains uni OR uni contains metrics_key OR basename matches)\n",
    "    for k, v in metrics_dict.items():\n",
    "        if not isinstance(k, str):\n",
    "            continue\n",
    "        k_str = k.strip()\n",
    "        if uni in k_str or k_str in uni or (uni_base and uni_base in k_str) or (basename(k_str) == uni_base):\n",
    "            return v\n",
    "\n",
    "    # fallback\n",
    "    return (0, 0, 0)\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# MAIN DATASET CREATION FUNCTION\n",
    "# ----------------------------\n",
    "def create_unified_dataset(project_name):\n",
    "    print(f\"\\nüîç Processing project: {project_name}\")\n",
    "\n",
    "    # 1Ô∏è‚É£ Gather universal file set (from Java + Native sources)\n",
    "    all_files = gather_universal_file_set(project_name)\n",
    "    if not all_files:\n",
    "        print(f\"‚ö†Ô∏è No files found for {project_name}\")\n",
    "        return\n",
    "\n",
    "    # 2Ô∏è‚É£ Load smell frequencies (per smell type)\n",
    "    smell_data = {}\n",
    "    for smell_name, folder in SMELL_DIRS.items():\n",
    "        smell_path = os.path.join(folder, f\"{project_name}.csv\")\n",
    "        if not os.path.exists(smell_path):\n",
    "            print(f\"‚ö†Ô∏è Missing smell file: {smell_path}\")\n",
    "            smell_data[smell_name] = {}\n",
    "        else:\n",
    "            smell_data[smell_name] = load_smell_counts(smell_path)\n",
    "\n",
    "    # 3Ô∏è‚É£ Load faulty files (set)\n",
    "    faulty_path = os.path.join(FAULTY_DIR, f\"{project_name}.csv\")\n",
    "    faulty_files = load_file_list(faulty_path)\n",
    "\n",
    "    # 4Ô∏è‚É£ Load metrics dictionary\n",
    "    metrics_path = os.path.join(METRICS_DIR, f\"{project_name}.csv\")\n",
    "    metrics_dict = load_metrics(metrics_path)\n",
    "\n",
    "    # 5Ô∏è‚É£ Combine everything (use containment-based matching)\n",
    "    dataset = []\n",
    "    for f in all_files:\n",
    "        # Smell frequencies per type using containment matching\n",
    "        smell_freqs = {s: smell_count_by_containment(smell_data.get(s, {}), f) for s in SMELL_DIRS.keys()}\n",
    "        total_freq = sum(smell_freqs.values())\n",
    "        if total_freq == 0:\n",
    "            continue  # keep only smelly files (same as original behavior)\n",
    "\n",
    "        loc, code_churn, previous_fixes = find_metrics_by_containment(metrics_dict, f)\n",
    "        fault_prone = 1 if is_faulty_by_containment(faulty_files, f) else 0\n",
    "\n",
    "        row = {\n",
    "            \"file\": f,\n",
    "            **smell_freqs,\n",
    "            \"LOC\": loc,\n",
    "            \"Code Churn\": code_churn,\n",
    "            \"Previous Bugs\": previous_fixes,\n",
    "            \"fault_prone\": fault_prone\n",
    "        }\n",
    "        dataset.append(row)\n",
    "\n",
    "    # 6Ô∏è‚É£ Save dataset\n",
    "    if not dataset:\n",
    "        print(f\"‚ö†Ô∏è No smelly files found for {project_name}\")\n",
    "        return\n",
    "\n",
    "    df_final = pd.DataFrame(dataset)\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    output_path = os.path.join(OUTPUT_DIR, f\"{project_name}.csv\")\n",
    "    df_final.to_csv(output_path, index=False)\n",
    "    print(f\"‚úÖ Saved: {output_path} ({len(df_final)} smelly files)\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# MAIN SCRIPT ENTRY POINT\n",
    "# ----------------------------\n",
    "def main():\n",
    "    java_files = []\n",
    "    for folder in UNIVERSAL_FOLDERS:\n",
    "        if os.path.exists(folder):\n",
    "            java_files += [f for f in os.listdir(folder) if f.endswith(\".csv\")]\n",
    "\n",
    "    # Deduplicate project names (same project may appear in multiple folders)\n",
    "    projects = list(set(os.path.splitext(f)[0] for f in java_files))\n",
    "\n",
    "    for project_name in sorted(projects):\n",
    "        create_unified_dataset(project_name)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c8a06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine releasewise dataset into project wise\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------\n",
    "INPUT_DIR = \"new smells/On extended dataset/release wise fault dataset\"        # Folder with release-wise CSVs\n",
    "OUTPUT_DIR = \"new smells/On extended dataset/project level fault datasets\"       # Folder for merged project-wise CSVs\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# HELPER FUNCTION\n",
    "# ----------------------------\n",
    "def extract_project_name(filename):\n",
    "    \"\"\"\n",
    "    Extracts the base project name (letters only, before any digits or underscores).\n",
    "    e.g., 'conscrypt1_unified_regression.csv' -> 'conscrypt'\n",
    "          'rocksdb_2_unified_regression.csv' -> 'rocksdb'\n",
    "    \"\"\"\n",
    "    base = os.path.splitext(filename)[0]\n",
    "    # remove suffix like \"_unified_regression\" if present\n",
    "    base = re.sub(r\"_unified_regression$\", \"\", base)\n",
    "    # take only the alphabetic prefix as project name\n",
    "    match = re.match(r\"([A-Za-z_-]+)\", base)\n",
    "    return match.group(1).lower() if match else base.lower()\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# MAIN MERGE FUNCTION\n",
    "# ----------------------------\n",
    "def merge_release_datasets():\n",
    "    all_files = [f for f in os.listdir(INPUT_DIR) if f.endswith(\".csv\")]\n",
    "    project_groups = {}\n",
    "\n",
    "    # Group files by project name\n",
    "    for file in all_files:\n",
    "        project = extract_project_name(file)\n",
    "        project_groups.setdefault(project, []).append(file)\n",
    "\n",
    "    print(f\"üîç Found {len(project_groups)} projects to merge.\\n\")\n",
    "\n",
    "    # Merge each project's releases\n",
    "    for project, files in project_groups.items():\n",
    "        print(f\"üìÇ Merging {len(files)} releases for project: {project}\")\n",
    "\n",
    "        combined_df = pd.DataFrame()\n",
    "\n",
    "        for file in sorted(files):\n",
    "            file_path = os.path.join(INPUT_DIR, file)\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                df[\"Release_File\"] = file  # optional, track origin release\n",
    "                combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error reading {file}: {e}\")\n",
    "\n",
    "        if not combined_df.empty:\n",
    "            output_path = os.path.join(OUTPUT_DIR, f\"{project}_all.csv\")\n",
    "            combined_df.to_csv(output_path, index=False)\n",
    "            print(f\"‚úÖ Saved merged dataset: {output_path} ({len(combined_df)} rows)\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è No data merged for {project} (empty CSVs).\")\n",
    "\n",
    "    print(\"\\nüéâ All project-level datasets created successfully!\")\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# RUN SCRIPT\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    merge_release_datasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61da6c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42654d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from statsmodels.tools.sm_exceptions import PerfectSeparationError\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------\n",
    "INPUT_DIR = \"new smells/On extended dataset/project level fault datasets\"   # Folder with merged project datasets\n",
    "OUTPUT_CSV = \"new smells/On extended dataset/projectwise_logistic_results_fault.csv\"\n",
    "\n",
    "# Columns to use in regression\n",
    "SMELL_COLS = [\"LanguageEnvy\", \"CrossRespDecl\", \"ShotgunSurgery\", \"EncapsulationBypass\"]\n",
    "#METRIC_COLS = [\"LOC\", \"Code Churn\", \"Previous Bugs\"]\n",
    "METRIC_COLS = []\n",
    "\n",
    "TARGET_COL = \"fault_prone\"\n",
    "\n",
    "# ----------------------------\n",
    "# HELPER FUNCTION\n",
    "# ----------------------------\n",
    "def drop_zero_variance_columns(df, cols):\n",
    "    \"\"\"Drop columns that have no variation (all values same).\"\"\"\n",
    "    drop_cols = [c for c in cols if df[c].nunique() <= 1]\n",
    "    if drop_cols:\n",
    "        print(f\"‚ö†Ô∏è Dropping constant columns: {drop_cols}\")\n",
    "        df = df.drop(columns=drop_cols)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# MAIN REGRESSION FUNCTION\n",
    "# ----------------------------\n",
    "def run_logistic_regression(project_name, df):\n",
    "    # Drop rows with missing data\n",
    "    df = df.dropna(subset=SMELL_COLS + METRIC_COLS + [TARGET_COL])\n",
    "\n",
    "    # Ensure binary target\n",
    "    if df[TARGET_COL].nunique() < 2:\n",
    "        print(f\"‚ö†Ô∏è Skipping {project_name}: target variable not binary.\")\n",
    "        return None\n",
    "\n",
    "    # Drop constant (zero variance) predictors\n",
    "    df = drop_zero_variance_columns(df, SMELL_COLS + METRIC_COLS)\n",
    "\n",
    "    # Define independent & dependent variables\n",
    "    predictors = [c for c in df.columns if c in SMELL_COLS + METRIC_COLS]\n",
    "    X = df[predictors]\n",
    "    y = df[TARGET_COL]\n",
    "\n",
    "    # Add intercept\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    # Try logistic regression (statsmodels)\n",
    "    try:\n",
    "        model = sm.Logit(y, X)\n",
    "        result = model.fit(disp=False)\n",
    "    except (PerfectSeparationError, np.linalg.LinAlgError, ValueError) as e:\n",
    "        print(f\"‚ö†Ô∏è Logistic regression failed for {project_name}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Predict to compute performance\n",
    "    df[\"pred_prob\"] = result.predict(X)\n",
    "    df[\"pred_label\"] = (df[\"pred_prob\"] >= 0.5).astype(int)\n",
    "    acc = accuracy_score(y, df[\"pred_label\"])\n",
    "    prec = precision_score(y, df[\"pred_label\"], zero_division=0)\n",
    "    rec = recall_score(y, df[\"pred_label\"], zero_division=0)\n",
    "    f1 = f1_score(y, df[\"pred_label\"], zero_division=0)\n",
    "\n",
    "    pseudo_r2 = 1 - (result.llf / result.llnull)\n",
    "\n",
    "    # Identify statistically significant variables (p < 0.05)\n",
    "    sig_vars = result.pvalues[result.pvalues < 0.05].index.tolist()\n",
    "    if \"const\" in sig_vars:\n",
    "        sig_vars.remove(\"const\")\n",
    "\n",
    "    print(f\"\\n‚úÖ {project_name} Regression Summary\")\n",
    "    print(f\"  Accuracy: {acc:.3f} | F1: {f1:.3f} | Pseudo-R¬≤: {pseudo_r2:.3f}\")\n",
    "    print(f\"  Significant predictors (p < 0.05): {sig_vars if sig_vars else 'None'}\")\n",
    "    print(result.summary())\n",
    "\n",
    "    # Store results\n",
    "    res = {\n",
    "        \"Project\": project_name,\n",
    "        \"NumFiles\": len(df),\n",
    "        \"Accuracy\": acc,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": rec,\n",
    "        \"F1\": f1,\n",
    "        \"Pseudo_R2\": pseudo_r2,\n",
    "        \"Significant_Predictors\": \", \".join(sig_vars) if sig_vars else \"None\"\n",
    "    }\n",
    "\n",
    "    # Add coefficients and p-values\n",
    "    for param, coef in result.params.items():\n",
    "        res[f\"Coef_{param}\"] = coef\n",
    "    for param, pval in result.pvalues.items():\n",
    "        res[f\"Pval_{param}\"] = pval\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# MAIN EXECUTION\n",
    "# ----------------------------\n",
    "def main():\n",
    "    results = []\n",
    "\n",
    "    for file in sorted(os.listdir(INPUT_DIR)):\n",
    "        if not file.endswith(\".csv\"):\n",
    "            continue\n",
    "        project_name = file.replace(\"_all.csv\", \"\")\n",
    "        print(f\"\\nüîç Running regression for: {project_name}\")\n",
    "\n",
    "        df = pd.read_csv(os.path.join(INPUT_DIR, file))\n",
    "        if df.empty:\n",
    "            print(f\"‚ö†Ô∏è Skipping {project_name}: empty dataset.\")\n",
    "            continue\n",
    "\n",
    "        res = run_logistic_regression(project_name, df)\n",
    "        if res:\n",
    "            results.append(res)\n",
    "\n",
    "    if results:\n",
    "        out_df = pd.DataFrame(results)\n",
    "        out_df.to_csv(OUTPUT_CSV, index=False)\n",
    "        print(f\"\\nüìä Saved logistic regression summary: {OUTPUT_CSV}\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå No successful regressions.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bed30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined data logistic regression\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Path to your project-level datasets\n",
    "base_dir = \"new smells/On extended dataset/project level fault datasets\"\n",
    "\n",
    "# Collect all CSV files\n",
    "project_dfs = []\n",
    "for file in os.listdir(base_dir):\n",
    "    if file.endswith(\".csv\"):\n",
    "        project_name = file.replace(\".csv\", \"\")\n",
    "        df = pd.read_csv(os.path.join(base_dir, file))\n",
    "        \n",
    "        # Ensure project column exists\n",
    "        df[\"project\"] = project_name\n",
    "        \n",
    "        # Keep only relevant columns (adjust if needed)\n",
    "        expected_cols = [\"LanguageEnvy\", \"CrossRespDecl\", \"ShotgunSurgery\", \n",
    "                         \"EncapsulationBypass\", \"fa_prone\", \"project\"]\n",
    "        df = df[[col for col in expected_cols if col in df.columns]]\n",
    "        \n",
    "        # Drop rows with missing values (if any)\n",
    "        df.dropna(subset=[\"change_prone\"], inplace=True)\n",
    "        \n",
    "        # Keep only binary 0/1 fault_prone\n",
    "        df = df[df[\"change_prone\"].isin([0, 1])]\n",
    "        \n",
    "        project_dfs.append(df)\n",
    "\n",
    "# Combine all project data into one DataFrame\n",
    "all_data = pd.concat(project_dfs, ignore_index=True)\n",
    "\n",
    "print(f\"‚úÖ Combined dataset shape: {all_data.shape}\")\n",
    "print(f\"üìä Projects included: {all_data['project'].unique().tolist()}\")\n",
    "\n",
    "# Convert categorical project variable\n",
    "all_data[\"project\"] = all_data[\"project\"].astype(\"category\")\n",
    "\n",
    "# Optional normalization for predictors (per project)\n",
    "for col in [\"LanguageEnvy\", \"CrossRespDecl\", \"ShotgunSurgery\", \"EncapsulationBypass\"]:\n",
    "    if col in all_data.columns:\n",
    "        all_data[col] = all_data.groupby(\"project\")[col].transform(\n",
    "            lambda x: (x - x.mean()) / x.std(ddof=0) if x.std(ddof=0) != 0 else 0\n",
    "        )\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# üìò Global Logistic Regression (with project as fixed effect)\n",
    "# -------------------------------------------------------------\n",
    "formula = \"change_prone ~ LanguageEnvy + CrossRespDecl + ShotgunSurgery + EncapsulationBypass + C(project)\"\n",
    "model = smf.logit(formula=formula, data=all_data).fit()\n",
    "\n",
    "print(\"\\n\\nüîç GLOBAL LOGISTIC REGRESSION RESULTS\")\n",
    "print(model.summary())\n",
    "\n",
    "# Save coefficients and p-values\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Variable\": model.params.index,\n",
    "    \"Coefficient\": model.params.values,\n",
    "    \"p-value\": model.pvalues.values,\n",
    "    \"Odds Ratio\": model.params.apply(lambda x: round(np.exp(x), 3))\n",
    "})\n",
    "\n",
    "# Filter only smell variables for interpretability\n",
    "smell_vars = [\"LanguageEnvy\", \"CrossRespDecl\", \"ShotgunSurgery\", \"EncapsulationBypass\"]\n",
    "significant_vars = summary_df[summary_df[\"Variable\"].isin(smell_vars) & (summary_df[\"p-value\"] < 0.05)]\n",
    "\n",
    "print(\"\\n\\nüìà SIGNIFICANT PREDICTORS (p < 0.05):\")\n",
    "print(significant_vars if not significant_vars.empty else \"None\")\n",
    "\n",
    "# Save the results to a CSV file\n",
    "output_path = \"new smells/On extended dataset/global_logistic_change_results.csv\"\n",
    "summary_df.to_csv(output_path, index=False)\n",
    "print(f\"\\nüíæ Results saved to: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
